---
title: 'Adam vs AdamW'
date: 2025-12-23
permalink: /posts/2025/12/blog-post-2/
use_math: true
tags:
  - optimization
  - ml
---

Adam and AdamW are two popular stochastic gradient descent variants used in machine learning repositories around the world. AdamW is supposedly better for complex models that could overfit but is it really the case ?

Reminder on the SGD
======
The classical SGD algorithm is an optimisation algorithm so easy to use if you have access to a gradient (also called oracle of order 1). The idea is just to follow the opposite direction to the gradient $g = \nabla f$ which will mechanically decrease the value of the loss function $f$. It's like going down a mountain, you want to follow the slope to be sure you're going the right way. However, you also need to decide your pace and if you're too fast, you may miss the bottom. That's why we also need in ML a learning rate to go not too fast towards the bottom and effectively minimize $L$. Hence at each step, we do something along the line of $$\theta_{t+1} = \theta_{t} + \alpha g_t.$$ 

A theoretical comparison
======
Adam
------
The idea of Adam is to reduce the learning rate adaptively on the gradients. Indeed, you usually have to go fast at first because the initialization of the weights is too random to be close to an optima but when you get close to a optima, you may miss it if you keep the same learning rate so you usually need to decrease it. One way is to use schedulers (ew, who wants to waste an afternoon tuning that ?) but doing it adaptively is way more robust if you change regularly the task, the loss, the data and you don't tune the hyperparameters. That's why Adam is designed to adaptively reduce the learning rate based on what happens during the optimization and the loss $L$ and it is the main reason why it became a standard.

AdamW
------
Now, AdamW is a variant of Adam where the way you add weight decay (to avoid overfitting) is different. Instead of changing the loss and adding a penalty to the loss (as it is done in the optimization litterature), you only change the parameter update. This means the modification of the learning rate stays the same as before, the only interference of the penalty induced by the weight decay is on the update. It is not so clear this algorithm is actually useful because Adam is constantly changing the learning rate based on the optimized loss so if you don't take into account the penalty in the learning rate modification, you might miss the bottom by going too fast.   

Test on a toy example
======
In this section, I consider only a toy example with a very simple 1d sine curve. There are two types of overfitting we can observe with this curve. The overfitting which makes the model miss the pattern and not generalize well out of distribution (let's call it OOD generalization) but also the overfitting which makes the model learn patterns in the noise of the data instead of finding an harmonized way to represent the data (let's call it ID generalization). The latter is usually what people refer to as overfitting but let's try both so that we are clear about the comparison Adam vs AdamW in all overfitting contexts. The following curves are obtained using 5 different optimizations (i.e. different initializations), the code is available [here](/files/adamvadamw.ipynb).

OOD generalization
-----
We train a MLP for $5000$ epochs on a part of the curve and we evaluate the result on the full sine curve. This way, we are able to check the ability of the model to generalize out of the training distribution. Without much surprise, the model is not able to generaliza well but if we add some weight decay, the result is better. No difference can be observed between Adam and AdamW.
![test OOD de Adam vs AdamW](/files/images/23_12_2025_1.png)

ID generalization
-----
We train a Unet model for $5000$ epochs on a noisy modification of the curve to test Adam and AdamW. In that case, Adam is the only method to show a difference when we add some weight decay. 
![test ID de Adam vs AdamW](/files/images/23_12_2025_2.png)

Conclusion
======
As we can see, AdamW is not really useful in all cases compared to Adam and it probably depend a lot on the task at hand. 